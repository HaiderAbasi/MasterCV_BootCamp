{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSpvEqlF6ZrvOC5ppnA6tF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"a3J7apVBe1lP"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"x_swEwsNewqJ"},"outputs":[],"source":["import cv2\n","import numpy as np\n","from loguru import logger\n","\n","from src.utilities import build_montages,print_h"]},{"cell_type":"code","source":["o_s = 100"],"metadata":{"id":"i9L_uvmBfAeM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def on_overlayS_change(val)"],"metadata":{"id":"SnXuAlEBfC6H"}},{"cell_type":"code","source":["def on_overlayS_change(val):\n","    global o_s\n","    o_s = val"],"metadata":{"id":"kFkodSP-fHM1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def img_registration(obj,scene,debug = True)"],"metadata":{"id":"kW1W71XVfJXj"}},{"cell_type":"code","source":["def img_registration(obj,scene,debug = True):\n","    #  Hint     : Find Homography will be be critical here.\n","\n","    # Write Code here\n","    obj_reg_on_scene = scene.copy()\n","\n","\n","    return obj_reg_on_scene"],"metadata":{"id":"5BaBIHrBfO0d"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def assignment(debug = True)"],"metadata":{"id":"1xIbriY7fSKk"}},{"cell_type":"code","source":["def assignment(debug = True):\n","    # Assignment: Orthomosaic is a composite image that is created by stitching together multiple overlapping\n","    #             images of an area taken from the same perspective. The images are geometrically corrected so\n","    #             that the scale is uniform across the entire image. This process is known as orthorectification.\n","    #             Here we have an orthomosaic and a distorted image taken by a drone from some part of the area\n","    #             of which the orthomosaic was generated from. But at some other time.\n","    #  Task     : Your task is to use the orthomosaic to correct the distortions in the distorted image and\n","    #             overlay it on the orthomosaic.(This will be useful in analyzig the recent changes that had\n","    #                                             had happened in the area)\n","    #\n","    #  Returns  : (img) drone view mapped on the mosaic using image registration.\n","    #\n","    #  Hint     : See Image registration in this regard.\n","    #             Reference: https://www.mathworks.com/discovery/image-registration.html#:~:text=Image%20registration%20is%20an%20image,are%20common%20when%20overlaying%20images.\n","    #                        Further read: https://analyticsindiamag.com/what-is-image-registration-and-how-does-it-work/\n","    print_h(\"[Assignment]:  Estimate the relative drone pose (utilizing the the drone view with known map using feature detection and mapping)\\n\")\n","\n","    # Input\n","    drone_view = cv2.imread(\"Data/test\\DSC00153.JPG\")\n","\n","    map = cv2.imread(\"Data/test/building_mosaic.tif\")\n","\n","    if debug:\n","        cv2.namedWindow(\"> drone_view < \",cv2.WINDOW_NORMAL)\n","        cv2.imshow(\"> drone_view < \",drone_view)\n","        cv2.waitKey(0)\n","        cv2.destroyWindow(\"> drone_view < \")\n","\n","    images = []\n","    titles = []\n","    images.append(drone_view)\n","    titles.append(\"drone_view\")\n","\n","\n","    # Task Function\n","    img_mapped_on_map = img_registration(drone_view,map,debug)\n","\n","    if (np.array_equal(img_mapped_on_map,map)):\n","        logger.error(\"img_registration() needs to be coded to get the required(distorted image registered on the orthomosaic) result.\")\n","        exit(0)\n","\n","\n","    # Output (Display)\n","    if debug:\n","        # Close previously opened windows\n","        cv2.destroyAllWindows()\n","        images.append(img_mapped_on_map)\n","        titles.append(\"img_mapped_on_map\")\n","        montage = build_montages(images,None,None,titles,True,True)\n","        for montage_img in montage:\n","            cv2.imshow(\"Image-registration\",montage_img)\n","        cv2.waitKey(0)\n","\n","    return img_mapped_on_map"],"metadata":{"id":"C3jl0tIhfWFM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def vis_keypoints(img"],"metadata":{"id":"i6b_g38ZfkZo"}},{"cell_type":"code","source":["def vis_keypoints(img):\n","    images = []\n","    titles = []\n","\n","    images.append(img)\n","    titles.append(\"deans_car\")\n","\n","    # A) Harris Corner detector : The first keypoint detector available in OpencV\n","    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) # grayscale and float32 type.\n","    blockSize = 2 # size of neighbourhood considered for corner detection\n","    ksize = 7     # Aperture parameter of the Sobel derivative used.\n","    k = 0.08      # Harris detector free parameter in the equation.\n","    # Cornerness = R = det (M) - k * (trace (M))^2\n","    dst = cv2.cornerHarris(gray,blockSize,ksize,k) # dst [Float_32 size same as src]\n","    dst = cv2.dilate(dst,None)\n","    # Threshold for an optimal value, it may vary depending on the image.\n","    img_harris = img.copy()\n","    img_harris[dst>0.01*dst.max()]=[0,0,255]\n","\n","    images.append(img_harris)\n","    titles.append(\"Corners (Harris)\")\n","\n","    # B) Shi-Thomsi Corner detector (Important: Improved Harris Corner so better to use this!)\n","    max_corners = 25   # Maximum numbers of corner you wish to get\n","    min_quality = 0.01 # Minimum quality required to be considered a valid corner Range (0-1)\n","    min_euc_dist = 10  # Minimum allowed euc distance between two corners\n","    corners = cv2.goodFeaturesToTrack(gray,max_corners,min_quality,min_euc_dist) # Returns detected corners (Float)\n","\n","    img_shi_thomsi = img.copy()\n","    corners = np.int0(corners) # Convert corners to int for display\n","    for i in corners:\n","        x,y = i.ravel() # unpack\n","        img_shi_thomsi = cv2.circle(img_shi_thomsi,(x,y),8,(255,0,0),-1)\n","\n","    images.append(img_shi_thomsi)\n","    titles.append(\"Corners (Shi-Thomsi)\")\n","\n","    # Displaying image and threshold result\n","    montage = build_montages(images,None,None,titles,True,True)\n","    for montage_img in montage:\n","        #imshow(\"Found Clusters\",cluster,cv2.WINDOW_AUTOSIZE)\n","        cv2.imshow(\"Keypoints\",montage_img)\n","    cv2.waitKey(0)"],"metadata":{"id":"iAr_Jahvflyb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def vis_features(img)"],"metadata":{"id":"9o5SoJNgftKa"}},{"cell_type":"code","source":["def vis_features(img):\n","    # SURF and SIFT are very robust, and perform well under scale and rotation variances. Affine shifts are a little tricky, but not bad. And FAST is not a descriptor, it is just a (mind-boggling fast!) detector.\n","    # If you're considering eligibility for real-time tests, then I'm afraid you'll have to trade-off a great deal of performance. SIFT and SURF are not real-time. Others are relatively faster (BRISK should top it, if I recall)\n","    images = []\n","    titles = []\n","\n","    images.append(img)\n","    titles.append(\"deans_car\")\n","\n","\n","    img_sift = img.copy()\n","\n","    #cv.SIFT_create(nfeatures, nOctaveLayers, contrastThreshold, edgeThreshold, sigma)\n","    # n features = Number of best features that you wish to retrieve\n","    # n OctaveLayers = Number of guassian layers in each octave\n","    # contrast threshold: Part of Keypoint localization: Where we remove low contrast candidates\n","    # edge threshold: Part of // // Where we remove candidates greater then the threshold\n","    # sigma : Sigma of guassian applied to input image at OCtave 0\n","    sift = cv2.SIFT_create()\n","\n","    # detect (InputArray image, std::vector< KeyPoint > &keypoints, InputArray mask=noArray())\n","    kp  = sift.detect(img,None)\n","\n","    cv2.drawKeypoints(img,kp,img_sift,flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS|cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","\n","    # dcptr, kp_updated = compute(image,KeyPoint)\n","    [descriptors, keypoints] = sift.compute(img,kp)\n","\n","    images.append(img_sift)\n","    titles.append(\"Sift\")\n","\n","    img_orb = img.copy()\n","    # Initiate ORB detector\n","    # scaleFactor\tPyramid decimation ratio\n","    # nlevels\tThe number of pyramid levels.\n","    # nfeatures\tThe maximum number of features to retain.\n","    orb = cv2.ORB_create()\n","    # find the keypoints and descriptors with ORB\n","    # kp, desc = detectAndCompute ( image,  mask, keypoints)\n","    kp1, des1 = orb.detectAndCompute(img,None)\n","    # draw only keypoints location,not size and orientation\n","    cv2.drawKeypoints(img, kp1, img_orb, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS|cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)\n","    images.append(img_orb)\n","    titles.append(\"ORB\")\n","\n","    # Displaying image and threshold result\n","    montage = build_montages(images,None,None,titles,True,True)\n","    for montage_img in montage:\n","        #imshow(\"Found Clusters\",cluster,cv2.WINDOW_AUTOSIZE)\n","        cv2.imshow(\"Features\",montage_img)\n","    cv2.waitKey(0)"],"metadata":{"id":"d5AGm2S0fxEs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def find_obj_inscene(obj,scene,method=\"sift\",debug=True,min_match_count = 15)"],"metadata":{"id":"2vStBu5Vf3mD"}},{"cell_type":"code","source":["def find_obj_inscene(obj,scene,method=\"sift\",debug=True,min_match_count = 15):\n","\n","    images = []\n","    titles = []\n","\n","    # Extract features\n","    if method ==\"orb\":\n","        if debug:\n","            print(\"\\n> Using ORB for detection and matching\")\n","        orb = cv2.ORB_create()\n","        # find the keypoints and descriptors with ORB\n","        kp1, des1 = orb.detectAndCompute(obj,None)\n","        kp2, des2 = orb.detectAndCompute(scene,None)\n","    elif method ==\"sift\":\n","        if debug:\n","            print(\"\\n> Using Sift for detection and matching\")\n","        sift = cv2.SIFT_create()\n","        # find the keypoints and descriptors with SIFT\n","        kp1, des1 = sift.detectAndCompute(obj,None)\n","        kp2, des2 = sift.detectAndCompute(scene,None)\n","    else:\n","        print(f\"Unknown method specified = {method}\")\n","        return\n","\n","    # Feature Matching\n","    if method == \"orb\":\n","        # create BFMatcher drone_viewect\n","        # 1) normType =  Normalization method used : Norm_hamming is recommend for Orb feature extractor\n","        # 2) crossCheck = crosscheck the matches: Set to true If not using Lowe's ratio test.\n","        bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n","        # Match descriptors.\n","        matches = bf.match(des1,des2) # Finds the best match for each desriptor in the feature set\n","        # Sort them in the order of their distance.\n","        matches = sorted(matches, key = lambda x:x.distance)\n","\n","        # Take first 20 matches.\n","        good = matches[:20]\n","\n","    elif method == \"sift\":\n","        # BFMatcher with default params\n","        bf = cv2.BFMatcher()\n","        matches = bf.knnMatch(des1,des2,k=2) # Finds the k best matches for each descriptor : Set k = 2 for\n","                                             #                                                for Lowe ratio test\n","        # Apply Lowe's ratio test\n","        good = []\n","        for m,n in matches:\n","            if m.distance < 0.75*n.distance:\n","                good.append(m)\n","\n","\n","    # Retreiving only matches that are actually part of object in scene\n","    M = None\n","    if debug:\n","        print(len(good))\n","    MIN_MATCH_COUNT = min_match_count\n","    if len(good)>MIN_MATCH_COUNT:\n","        src_pts = np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)\n","        dst_pts = np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)\n","        M, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n","        matchesMask = mask.ravel().tolist()\n","        h,w = obj.shape[0:2]\n","        pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)\n","        dst = cv2.perspectiveTransform(pts,M)\n","        scene = cv2.polylines(scene,[np.int32(dst)],True,255,3, cv2.LINE_AA)\n","    else:\n","        print( \"Not enough matches are found - {}/{}\".format(len(good), MIN_MATCH_COUNT) )\n","        matchesMask = None\n","\n","\n","    draw_params = dict(matchColor = (0,255,0), # draw matches in green color\n","                    singlePointColor = None,\n","                    matchesMask = matchesMask, # draw only inliers\n","                    flags = 2)\n","    matched_img = cv2.drawMatches(obj,kp1,scene,kp2,good,None,**draw_params)\n","\n","    images.append(matched_img)\n","    titles.append(f\"Feature Matching ({method})\")\n","\n","    if debug:\n","        # Displaying image and threshold result\n","        montage = build_montages(images,None,None,titles,False,True)\n","        for montage_img in montage:\n","            method_str = f\"({method})\"\n","            cv2.imshow(\"find_Obj_in_Scene \" + method_str.upper(),montage_img)\n","        cv2.waitKey(0)\n","\n","    return M"],"metadata":{"id":"tCqgabzEf7TW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def main()"],"metadata":{"id":"UgwE_nncgCHi"}},{"cell_type":"code","source":["def main():\n","    print_h(\"[main]: OpenCV Image feature-extraction and Usage.\")\n","\n","    # Task a : Keypoint Extraction\n","    print_h(\"[a]: Extracting and Visualizing keypoints and features of an image.\")\n","    img = cv2.imread(\"Data\\supernatural-impala.jpg\")\n","\n","    vis_keypoints(img)\n","    vis_features(img)\n","\n","    # Application: Finding known object in a scene\n","    print_h(\"[b]: Using feature-matching to find obj in scene.\")\n","    obj = cv2.imread(\"Data\\ltp.jpg\")\n","    scene = cv2.imread(\"Data\\scene2.jpg\")\n","\n","    find_obj_inscene(obj,scene,\"orb\")\n","\n","    find_obj_inscene(obj,scene,\"sift\")\n","\n","\n","    cv2.destroyAllWindows()"],"metadata":{"id":"Yh3X4QETgEtU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conditional Execution of Functions Based on Readiness Status"],"metadata":{"id":"o0-_jvn6gIQz"}},{"cell_type":"code","source":["if __name__ ==\"__main__\":\n","    i_am_ready = False\n","\n","    if i_am_ready:\n","        assignment()\n","    else:\n","        main()"],"metadata":{"id":"FXlZ-E86gKgI"},"execution_count":null,"outputs":[]}]}