{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNRwv2K3lHWxnK44rJSDM5z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["### Imports"],"metadata":{"id":"flbbBPlcigoX"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GF0hjrEniXFc"},"outputs":[],"source":["import cv2\n","import numpy as np\n","import time\n","import math\n","import os\n","from loguru import logger\n","\n","from src.utilities import putText,print_h,disp_fps"]},{"cell_type":"markdown","source":["### def assignment()"],"metadata":{"id":"s6KCnG83io-u"}},{"cell_type":"code","source":["def assignment():\n","\n","    print_h(\"[Assignment]:  Train a yolo-v7 to a custom dataset(traffic-signs) and use it to detect signs on the test-video.\\n\")\n","    # Use the following dataset or create your own.\n","    # \\/ Dataset (You can use) \\/\n","    # https://universe.roboflow.com/new-workspace-vl4lk/dataset-traffic\n","    # /\\                       /\\\n","\n","    # Input Video\n","    test_vid_path = \"Data\\dash_view_3.mp4\"\n","\n","    # Setting path to required files of obj-detection [Model,Weight,Classes]\n","    Dataset_dir = \"Data/dnn/yolo_v7/traffic-signs\"\n","    if (os.path.isdir(Dataset_dir)):\n","        model_path   = os.path.join(Dataset_dir,\"yolov7-tiny_TL.cfg\")\n","        weights_path = os.path.join(Dataset_dir,\"yolov7-tiny_TL_best.weights\")\n","        classes_path = os.path.join(Dataset_dir,\"traffic_signs.names\")\n","    else:\n","        training_in_colab = \"https://colab.research.google.com/drive/1ZsbiV62151gw2qwI3pdkqV3nojCx1T6x?usp=sharing#scrollTo=AqQaECSLJ8Yv\"\n","        logger.error(f\"\\n\\n> Train the yoloV7-tiny using the method described in...\\n\\n {training_in_colab}\\n\\n - traffics-sign dataset you will use...\\\n","                     \\n\\n    https://universe.roboflow.com/new-workspace-vl4lk/dataset-traffic\\n\")\n","        exit()\n","\n","    # Loading the Yolo-v7-tiny trained on a traffic-signs for testing on a video.\n","    detection_yolo = Dnn(model_path,weights_path,classes_path,0.4)\n","\n","    vid = cv2.VideoCapture(test_vid_path)\n","    # Object detection in video\n","    while(vid.isOpened()):\n","        ret,frame = vid.read()\n","\n","        if ret:\n","            detection_yolo.detect(frame)\n","            cv2.imshow('Road-Signs-detection',frame)\n","            k=cv2.waitKey(1)\n","            if k==27:\n","                break\n","        else:\n","            print(\"Video Ended\")\n","            break"],"metadata":{"id":"qejhRHKUisOf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### class Dnn"],"metadata":{"id":"a2gKWSP2ixTS"}},{"cell_type":"code","source":["class Dnn:\n","\n","    def __init__(self,model_path = None,weights_path = None,classes_path = None,conf = 0.75,colors = [(255,0,0,0,0,255,)]):\n","\n","        if model_path == None:\n","            model_path = r'Data\\Dnn\\yolov3.cfg'\n","        if weights_path==None:\n","            weights_path = r'Data\\Dnn\\yolov3.weights'\n","\n","        # Read a model stored in Darknet Model format\n","        self.net = cv2.dnn.readNetFromDarknet(model_path,weights_path)\n","        # Set Target Device for computations\n","        self.net.setPreferableTarget(cv2.dnn.DNN_TARGET_CPU) # Target Device For computation\n","\n","        # Set default detection classes to coco dataset path\n","        self.classes_path = r'Data/Dnn/coco.names'\n","\n","        # If the user specifies a classes_path, Use that\n","        if classes_path!= None:\n","            self.classes_path= classes_path\n","\n","        # Reading the classes object detector can detect\n","        self.classes = open(self.classes_path).read().strip().split('\\n')\n","\n","        np.random.seed(42)\n","        self.colors = np.random.randint(0, 255, size=(len(self.classes), 3), dtype='uint8')\n","\n","        #  Getting the output layers (82,94,106) names\n","        self.ln = self.net.getUnconnectedOutLayersNames()\n","\n","        # Setting the initial confidence parameter\n","        self.conf = conf\n","\n","\n","    def post_process(self,img,outputs,conf):\n","\n","        H, W = img.shape[:2]\n","        boxes = []\n","        confidences = []\n","        classIDs = []\n","\n","        # [bbox, Po , P_80_classes]\n","        for output in outputs:\n","            # Retrieve max confidence\n","            scores = output[5:]\n","            classID = np.argmax(scores)\n","            confidence = scores[classID]\n","            # Append as a detection if confidence greater then threshold\n","            if confidence >conf:\n","                x, y, w, h = output[:4] * np.array([W, H, W, H])\n","                p0 = int(x - w//2), int(y - h//2)\n","                p1 = int(x + w//2), int(y + h//2)\n","                boxes.append([*p0, int(w), int(h)])\n","                confidences.append(float(confidence))\n","                classIDs.append(classID)\n","\n","        # Stage 2: Removing overlapping region with IOU > threshold in NonMaxSuppression\n","        indices = cv2.dnn.NMSBoxes(boxes,confidences,conf,0.1)\n","\n","        det_objs_bboxes = []\n","        det_objs_classes = []\n","        # Draw detected bboxes along with their predicted class + confidence\n","        if len(indices)>0:\n","            for i in indices.flatten():\n","                # Drawing bbox\n","                (x, y) = (boxes[i][0], boxes[i][1])\n","                (w, h) = (boxes[i][2], boxes[i][3])\n","                color = [int(c) for c in self.colors[classIDs[i]]]\n","                cv2.rectangle(img, (x, y), (x + w, y + h), color, 4)\n","                # Display predicted class + confidence\n","                text = \"{}: {:.2f}\".format(self.classes[classIDs[i]], confidences[i])\n","                FONT_SCALE = 2e-3  # Adjust for larger font size in all images\n","                THICKNESS_SCALE = 1e-3\n","                height, width = img.shape[:2]\n","                font_scale = min(width,height)*FONT_SCALE\n","                thickness = math.ceil((min(width,height)*THICKNESS_SCALE))\n","                putText(img, text,  org= (x, y - 15),fontScale=font_scale,color=(255,0,255),thickness=thickness)\n","\n","                det_objs_bboxes.append([x,y,w,h])\n","                cls = self.classes[classIDs[i]]\n","                det_objs_classes.append(cls)\n","\n","        return det_objs_bboxes,det_objs_classes\n","\n","    def detect(self,frame):\n","\n","        start_time = time.time()\n","        # Creates 4-dimensional blob[images,channels,w,h] from image. requred as the input to yolo V3.\n","        # ScaleFactor = scale our images by some factor\n","        # size =  size that the Network(YoloV3) expects\n","        # SwapRB = boolean for swaping first <-> last channel\n","        # crop = Crop image after resizing T/F\n","        # blobFromImage(image,scaleFactor,Size,swapRb,crop)\n","        blob = cv2.dnn.blobFromImage(frame,1/255.0,(416,416),swapRB=True,crop = False)\n","\n","        # Sets the new input value for the network.\n","        self.net.setInput(blob)\n","        # Forward Propogation (Prediction Using object detector)\n","        # Yolo -> Stage 1 + Stage 3\n","        outputs = self.net.forward(self.ln)\n","        # Outputs:  vectors of lenght 85\n","        # [Po,bbox,P_80_classes]\n","\n","        # combine the 3 output groups into 1 (10647, 85)\n","        # large objects (507, 85)\n","        # medium objects (2028, 85)\n","        # small objects (8112, 85)\n","        outputs = np.vstack(outputs)\n","\n","        # Non-max Suprresion + Drawing predicted bbox and Class\n","        pred_bboxes,pred_classes = self.post_process(frame,outputs, self.conf)\n","\n","        # Displaying fps of yolo detections\n","        disp_fps(frame,start_time)\n","\n","        return pred_bboxes,pred_classes"],"metadata":{"id":"h3qgmAD4i1ne"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### def main()"],"metadata":{"id":"tDGxz3Aoi_hN"}},{"cell_type":"code","source":["def main():\n","\n","    print_h(\"[Main] Performing (obj) Detection using yolo-v3 [Dnn-module].\\n\")\n","\n","    # Create object of Dnn Class\n","    detection_yolo = Dnn()\n","\n","    # Object detection in video\n","    vid_path = r\"Data\\dash_view.webm\"\n","    vid = cv2.VideoCapture(vid_path)\n","\n","    while(vid.isOpened()):\n","        ret,frame = vid.read()\n","\n","        if ret:\n","            detection_yolo.detect(frame)\n","            cv2.imshow('Obj-detection (Yolo-V3)',frame)\n","            k=cv2.waitKey(1)\n","            if k==27:\n","                break\n","        else:\n","            print(\"Video Ended\")\n","            break\n","\n","    cv2.destroyWindow('Obj-detection (Yolo-V3)')\n","\n","    # Using the Yolo-v7-tiny trained on a custom dataset for soccer fans.\n","    model_path = r\"Data\\dnn\\yolo_v7\\football\\yolov7-tiny_TL.cfg\"\n","    weights_path = r\"Data\\dnn\\yolo_v7\\football\\yolov7-tiny_TL_best.weights\"\n","    classes_path = r\"Data\\dnn\\yolo_v7\\football\\football.names\"\n","    detection_yolo = Dnn(model_path,weights_path,classes_path,0.4)\n","\n","    # Object detection in video\n","    richarlson_goal = r\"Data\\richarlison_goal.mp4\"\n","    vid = cv2.VideoCapture(richarlson_goal)\n","\n","    while(vid.isOpened()):\n","        ret,frame = vid.read()\n","\n","        if ret:\n","            detection_yolo.detect(frame)\n","            cv2.imshow('Obj-detection (Yolo-V7-tiny)',frame)\n","            k=cv2.waitKey(1)\n","            if k==27:\n","                break\n","        else:\n","            print(\"Video Ended\")\n","            break"],"metadata":{"id":"w-__H2DOjDPE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Conditional Execution of Functions Based on Readiness Status"],"metadata":{"id":"78wymSpjjMXQ"}},{"cell_type":"code","source":["if __name__==\"__main__\":\n","\n","    i_am_ready = False\n","\n","    if i_am_ready:\n","        assignment()\n","    else:\n","        main()"],"metadata":{"id":"NsipZFkbjPDo"},"execution_count":null,"outputs":[]}]}